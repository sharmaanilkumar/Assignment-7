# -*- coding: utf-8 -*-
"""Assignment7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bGcNheXiUs9IAYHsbc_znG79DUjS-9hh

In this notebook I will show how to generate text with usage of Recurrent Neural Network. I will use Shakespare work for that exercise.

### Import
"""

import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM
from keras.optimizers import Adam

!pip install tqdm
from tqdm import tqdm
import urllib

from urllib.request import urlretrieve

import tensorflow as tf

import matplotlib.pyplot as plt
# %matplotlib inline

import os
import re
import random as rn
import numpy as np

# Randomness control
os.environ['PYTHONHASHSEED'] = '0'
RANDOM_SEED = 3939
np.random.seed(RANDOM_SEED)
rn.seed(RANDOM_SEED)
session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, 
                              inter_op_parallelism_threads=1)

tf.set_random_seed(RANDOM_SEED)
sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)
keras.backend.set_session(sess)

DATA_URL = "http://www.gutenberg.org/cache/epub/1041/pg1041.txt"
DATA_FILENAME = "sonnets.txt"

SEQ = 100
feature = 1

"""### Fetch Data"""

class DLProgress(tqdm):
  last_block = 0

  def hook(self, block_num=1, block_size=1, total_size=None):
    self.total = total_size
    self.update((block_num - self.last_block) * block_size)
    self.last_block = block_num

with DLProgress(unit="B", unit_scale=True, miniters=1, desc="Shakespeare's Sonnets") as pbar:
  urlretrieve(DATA_URL, DATA_FILENAME, pbar.hook)

"""### Load Data"""

with open(DATA_FILENAME, "r") as file:
  data = file.read()

start_index = 740
end_index = re.search("Love's fire heats water, water cools not love.", data)
end_index = end_index.end()

data_cleaned = data[start_index:end_index]
# print(data_cleaned)

"""To lower case"""

data_cleaned = data_cleaned.lower()

"""Number of characters"""

print(len(data_cleaned))

"""Reducing the size of dataset for saving computation time."""

split_index = int(0.5 * len(data_cleaned))
data_cleaned = data_cleaned[:split_index]

"""### Preprocessing the Dataset

Mapping every unique character to integer id
"""

characters = sorted(list(set(data_cleaned)))
id_to_character = {i:char for i, char in enumerate(characters)}
character_to_id = {char:i for i, char in enumerate(characters)}

totalchar = len(data_cleaned)
print(totalchar)



"""Creating the  input and output sequences"""

def data_to_sequence(data, data_to_id_dict):
  seq_Xs, seq_Ys = list(), list()

  for i in range(0, len(data) - SEQ):
    seq = data[i:i + SEQ]
    label = data[i + SEQ]
    
    seq_Xs.append([data_to_id_dict[char] for char in seq])
    seq_Ys.append(data_to_id_dict[label])
  
  return seq_Xs, seq_Ys

seq_Xs, seq_ys = data_to_sequence(data_cleaned, character_to_id)

for x, y in zip(seq_Xs[0:2], seq_ys[0:2]):
  print(x, y)

"""Assembling the  train_X, train_y"""

train_X = np.reshape(seq_Xs, (len(seq_Xs), SEQ, feature))
train_y = keras.utils.to_categorical(seq_ys)

"""Normalize"""

train_X = train_X / float(len(characters))

"""### Model

Structure
"""

model = Sequential()

model.add(LSTM(800, input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=True))
model.add(Dropout(0.2))

model.add(LSTM(800))
model.add(Dropout(0.2))

model.add(Dense(train_y.shape[1], activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer=Adam())

"""Training"""

history = model.fit(train_X, train_y, 
                    epochs = 20, 
                    batch_size = 128, 
                    verbose = 1, 
                    shuffle = False)

"""### Text generation"""

string_mapped = seq_Xs[100]
full_string = [id_to_character[value] for value in string_mapped]

for i in range(1000):
    x = np.reshape(string_mapped,(1,len(string_mapped), 1))
    x = x / float(len(characters))
    
    pred_index = np.argmax(model.predict(x, verbose=0))
    seq = [id_to_character[value] for value in string_mapped]
    full_string.append(id_to_character[pred_index])

    string_mapped.append(pred_index)
    string_mapped = string_mapped[1:len(string_mapped)]

generated_text = ""
for char in full_string:
    generated_text += char

print(generated_text)

